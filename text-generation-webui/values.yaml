replicaCount: 1
image:
  repository: atinoda/text-generation-webui
  pullPolicy: IfNotPresent
  #tag: default-nvidia #Enable this if using CUDA
  tag: default-cpu

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

service:
  type: ClusterIP
  webUiPort: 7860
  apiPort: 5000

persistence: # A pvc to store models, characters, config, etc.
  enabled: false  #If false, all storage will be dropped each time the pod is restarted
 # existingClaim: my-llm-data-pvc  # An existing PVC to use


gradioAuth:
  enabled: false
  secretName: gradio-auth

livenessProbe:
  enabled: false
  httpGet:
    path: /
    port: apiPort
readinessProbe:
  enabled: false
  httpGet:
    path: /
    port: apiPort

autoscaling:
  enabled: false


# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true


affinity: {}

#Will be passed into CMD_FLAGS.txt
additionalCmdLineParams:
  - "--listen"
  - "--api"  #Start the API on port 5000



tls:
  enabled: false
#  secretName: my-tls-secret



extra_env_vars:
  # the port the webui binds to on the host
  - name: HOST_PORT
    value: "7860"

  # the port the webui binds to inside the container
  - name: CONTAINER_PORT
    value: "7860"

  # the port the api binds to on the host
  - name: HOST_API_PORT
    value: "5000"

  # the port the api binds to inside the container
  - name: CONTAINER_API_PORT
    value: "5000"

  # Comma separated extensions to build
  - name: BUILD_EXTENSIONS
    value: ""



cuda: # Use CUDA to access NVIDIA GPU resources. Requires NVIDIA and Cuda drivers to be setup on the GPU node.
  #Be sure to change image.tag to an Nvidia docker image such as `default-nvidia`
  enabled: "true"
  visibleDevices: "all"
  capabilities: "all" #Needed for some flavors of Kubernetes
  torchCudaArchList: "7.5" #3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX https://developer.nvidia.com/cuda-gpus
  runtimeClassName: "nvidia"
  appRuntimeGID: "6972" #the group ID of the host user.

resources: {}
#  limits:
#    nvidia.com/gpu: 1

sleep: false

nodeSelector:
  "node-role.kubernetes.io/llm": llm


#tolerations: {}



serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""


podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

ingress:
  enabled: false


